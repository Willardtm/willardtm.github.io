<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>Skip-Gram模型与CBOW模型代码实现 | Klaus&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="词向量是NLP的基础，也是NLP学习中更重要的组成部分，在补基础的过程中我觉得这两个模型有必要自己学习并且写一遍才能更好的理解相关概念。Skip-Gram模型与CBOW模型出自于:Efficient Estimation of Word Representations inVector Space Mikolov大神是真的强，学习之。先前学习word2vec的学习笔记链接：cs224n-Lectu">
<meta name="keywords" content="NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="Skip-Gram模型与CBOW模型代码实现">
<meta property="og:url" content="http://yoursite.com/2019/10/18/Skip-Gram模型与CBOW模型代码实现/index.html">
<meta property="og:site_name" content="Klaus&#39;s Blog">
<meta property="og:description" content="词向量是NLP的基础，也是NLP学习中更重要的组成部分，在补基础的过程中我觉得这两个模型有必要自己学习并且写一遍才能更好的理解相关概念。Skip-Gram模型与CBOW模型出自于:Efficient Estimation of Word Representations inVector Space Mikolov大神是真的强，学习之。先前学习word2vec的学习笔记链接：cs224n-Lectu">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/18/KV7lXd.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/18/KVHLaq.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/18/KVHzzF.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/18/KVbkIx.png">
<meta property="og:updated_time" content="2019-10-18T07:24:54.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Skip-Gram模型与CBOW模型代码实现">
<meta name="twitter:description" content="词向量是NLP的基础，也是NLP学习中更重要的组成部分，在补基础的过程中我觉得这两个模型有必要自己学习并且写一遍才能更好的理解相关概念。Skip-Gram模型与CBOW模型出自于:Efficient Estimation of Word Representations inVector Space Mikolov大神是真的强，学习之。先前学习word2vec的学习笔记链接：cs224n-Lectu">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/10/18/KV7lXd.png">
  
    <link rel="alternate" href="/atom.xml" title="Klaus&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Klaus&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">MIA SAN MIA</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Skip-Gram模型与CBOW模型代码实现" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/18/Skip-Gram模型与CBOW模型代码实现/" class="article-date">
  <time datetime="2019-10-18T03:20:32.000Z" itemprop="datePublished">2019-10-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Skip-Gram模型与CBOW模型代码实现
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  <!-- Table of Contents -->
		
		  <div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#复现Word2Vec模型的步骤"><span class="toc-number">1.</span> <span class="toc-text">复现Word2Vec模型的步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Skip-Gram模型实现"><span class="toc-number">2.</span> <span class="toc-text">Skip-Gram模型实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CBOW模型实现"><span class="toc-number">3.</span> <span class="toc-text">CBOW模型实现</span></a></li></ol>
		  </div>
		
        <p>词向量是NLP的基础，也是NLP学习中更重要的组成部分，在补基础的过程中我觉得这两个模型有必要自己学习并且写一遍才能更好的理解相关概念。<br>Skip-Gram模型与CBOW模型出自于:<a href="https://arxiv.org/abs/1301.3781" target="_blank" rel="noopener">Efficient Estimation of Word Representations in<br>Vector Space</a> Mikolov大神是真的强，学习之。<br>先前学习word2vec的学习笔记链接：<br><a href="http://klausvon.cn/2019/08/08/cs224n-Lecture-2-%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%EF%BC%9Aword2vec/" target="_blank" rel="noopener">cs224n-Lecture-2 词向量表示：word2vec</a><br><a href="http://klausvon.cn/2019/08/14/cs224n-Lecture-3-%E9%AB%98%E7%BA%A7%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/" target="_blank" rel="noopener">cs224n-Lecture-3 高级词向量表示</a><br><a id="more"></a></p>
<p>在实践模型之前，我们需要知道两个关键的定义：词向量和词向量矩阵。<br>词向量：将一个词映射为一个向量，这个向量包含这个词的语法，语义的信息。<br>词向量矩阵：一个词向量就是一个行向量，把很多行向量排在一起就变成词向量矩阵。</p>
<h2 id="复现Word2Vec模型的步骤"><a href="#复现Word2Vec模型的步骤" class="headerlink" title="复现Word2Vec模型的步骤"></a>复现Word2Vec模型的步骤</h2><p>个人总结：<br>1.确定语料库(如果无法使用爬虫则直接下载);<br>2.文本预处理：<br>    (1)取出单词生成词汇表;<br>    (2)设置词汇表中原始单词的index；<br>    (3)统计单词出现次数，根据需要选择出现最多的num个单词来构建使用的新词表；<br>    (4)根据原始词表对新词表中的单词进行标号，生成一个正向词典和一个方向词典；<br>3.生成训练时需要的batch(cbow和sg的生成方式略有差别)；<br>4.设置tensorflow训练使用的参数(loss,optimizer etc.)；<br>5.开始训练。<br>代码链接：<a href="https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/examples/tutorials/word2vec/word2vec_basic.py" target="_blank" rel="noopener">word2vec</a></p>
<h2 id="Skip-Gram模型实现"><a href="#Skip-Gram模型实现" class="headerlink" title="Skip-Gram模型实现"></a>Skip-Gram模型实现</h2><p>Skip-Gram模型是通过中心词来推导出上下文词，结构如下：<br><img src="https://s2.ax1x.com/2019/10/18/KV7lXd.png" alt></p>
<p>具体定义和操作均在代码注释中,因为个人代码的书写习惯，对原始代码稍作改动：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line">from __future__ <span class="built_in">import</span> absolute_import</span><br><span class="line">from __future__ <span class="built_in">import</span> division</span><br><span class="line">from __future__ <span class="built_in">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> collections</span><br><span class="line"><span class="built_in">import</span> math</span><br><span class="line"><span class="built_in">import</span> os</span><br><span class="line"><span class="built_in">import</span> random</span><br><span class="line"><span class="built_in">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line">from six.moves <span class="built_in">import</span> urllib</span><br><span class="line">from six.moves <span class="built_in">import</span> xrange</span><br><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"><span class="built_in">import</span> matplotlib</span><br><span class="line">matplotlib.use('agg')</span><br><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line">from sklearn.manifold <span class="built_in">import</span> TSNE</span><br><span class="line"></span><br><span class="line"><span class="attr">url='http://mattmahoney.net/dc/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件下载模块</span></span><br><span class="line">def maybe_download(filename,expected_bytes):</span><br><span class="line">    <span class="keyword">if</span> not os.path.exists(filename):</span><br><span class="line">        print('start downloading...')</span><br><span class="line">        filename,<span class="attr">_=urllib.request.urlretrieve(url+filename,filename)</span></span><br><span class="line">    <span class="attr">statinfo=os.stat(filename)</span></span><br><span class="line">    <span class="keyword">if</span> statinfo.<span class="attr">st_size==expected_bytes:</span></span><br><span class="line">        print('Found <span class="literal">and</span> verified',filename)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(statinfo.st_size)</span><br><span class="line">        raise Exception('Failed to verify ' + filename + '. Can you get to it <span class="keyword">with</span> a browser?')</span><br><span class="line">    return filename</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压语料库并转换成一个word的list</span></span><br><span class="line">def read_data(filename):</span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(filename) as f:</span><br><span class="line">        <span class="attr">data=tf.compat.as_str(f.read(f.namelist()[0])).split()</span></span><br><span class="line">        <span class="comment"># tf.compat.as_str 将数据转为单词列表</span></span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原始的单词表示为index</span></span><br><span class="line">def build_data(words,n_words):</span><br><span class="line">    <span class="attr">count=[['UNK',-1]]</span></span><br><span class="line">    count.extend(collections.Counter(words).most_common(n_words-<span class="number">1</span>))</span><br><span class="line">    <span class="attr">dictionary=dict()</span></span><br><span class="line">    <span class="comment"># 生成单词及编号</span></span><br><span class="line">    <span class="comment"># 单词顺序是从出现次数最多的单词到出现次数较小的单词，逐次递减的</span></span><br><span class="line">    for word,_ <span class="keyword">in</span> count:</span><br><span class="line">        dictionary[word]=len(dictionary)</span><br><span class="line">    <span class="attr">data=list()</span></span><br><span class="line">    <span class="attr">unk_count=0</span></span><br><span class="line">    for word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">            <span class="attr">index=dictionary[word]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="attr">index=0</span> <span class="comment"># UNK的index值设置为0</span></span><br><span class="line">            unk_count+=<span class="number">1</span></span><br><span class="line">        <span class="comment"># data表是由单词出现的次数大小生成的</span></span><br><span class="line">        data.append(index)</span><br><span class="line">    <span class="comment"># print(unk_count)</span></span><br><span class="line">    count[<span class="number">0</span>][<span class="number">1</span>]=unk_count</span><br><span class="line">    <span class="attr">reversed_dictionary=dict(zip(dictionary.values(),dictionary.keys()))</span></span><br><span class="line">    <span class="comment"># data: 所有单词的编号集合</span></span><br><span class="line">    <span class="comment"># count: 每个单词及其出现的次数</span></span><br><span class="line">    <span class="comment"># dictionary: 词典，包含了出现最多的5w个单词中的单词和对应的编号</span></span><br><span class="line">    <span class="comment"># reversed_dictionary: 反向词典</span></span><br><span class="line">    return data,count,dictionary,reversed_dictionary</span><br><span class="line"></span><br><span class="line"><span class="attr">data_index=0</span></span><br><span class="line"><span class="comment"># next,定义一个函数用于生成skip-gram模型用的batch</span></span><br><span class="line"><span class="comment"># batch_size: 批量训练时数据量的大小</span></span><br><span class="line"><span class="comment"># num_skips: 词窗大小,假如词数是7,那么num_skips=6</span></span><br><span class="line"><span class="comment"># skip_window: 目标词临近的词窗大小</span></span><br><span class="line">def generate_batch(batch_size,num_skips,skip_window):</span><br><span class="line">    <span class="comment"># data_index 相当于一个指针,初始为0</span></span><br><span class="line">    <span class="comment"># 每生成一个batch_size,data_index就会相应地往后推</span></span><br><span class="line">    global data_index</span><br><span class="line">    <span class="comment"># 确保训练的次数是整数</span></span><br><span class="line">    <span class="keyword">assert</span> batch_size%<span class="attr">num_skips==0</span></span><br><span class="line">    <span class="comment"># 不能超过skip_window</span></span><br><span class="line">    <span class="keyword">assert</span> num_skips&lt;=skip_window*<span class="number">2</span></span><br><span class="line">    <span class="attr">batch=np.ndarray(shape=(batch_size),dtype=np.int32)</span></span><br><span class="line">    <span class="attr">labels=np.ndarray(shape=(batch_size,1),dtype=np.int32)</span></span><br><span class="line">    <span class="comment"># span: 上下文词的范围和目标词的大小</span></span><br><span class="line">    <span class="attr">span=2*skip_window+1</span></span><br><span class="line">    <span class="comment"># buffer: 缓冲区大小,用于采样的移动窗口,当有新的单词进入缓冲区时,最左边的单词就会从缓冲区排出，给新的单词让位</span></span><br><span class="line">    <span class="attr">buffer=collections.deque(maxlen=span)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># data_index 是当前数据开始的位置</span></span><br><span class="line">    <span class="comment"># 产生batch后就往后推1位</span></span><br><span class="line">    for _ <span class="keyword">in</span> range(span):</span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">        <span class="attr">data_index=(data_index+1)%len(data)</span></span><br><span class="line">    for i <span class="keyword">in</span> range(batch_size//num_skips):</span><br><span class="line">        <span class="comment"># 一个buffer生成num_skips个数的样本</span></span><br><span class="line">        <span class="comment"># target 是最中间的词</span></span><br><span class="line">        <span class="attr">target=skip_window</span></span><br><span class="line">        <span class="comment"># targets_to_avoid 保证样本不重复</span></span><br><span class="line">        <span class="attr">targets_to_avoid=[skip_window]</span></span><br><span class="line">        for j <span class="keyword">in</span> range(num_skips):</span><br><span class="line">            while target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">                <span class="attr">target=random.randint(0,span-1)</span></span><br><span class="line">            targets_to_avoid.append(target)</span><br><span class="line">            <span class="comment"># skip-gram 模型核心语句</span></span><br><span class="line">            batch[i*num_skips+j]=buffer[skip_window]</span><br><span class="line">            labels[i*num_skips+j,<span class="number">0</span>]=buffer[target]</span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">		<span class="attr">data_index=(data_index+1)%len(data)</span></span><br><span class="line">    <span class="attr">data_index=(data_index+len(data)-span)%len(data)</span></span><br><span class="line">    return batch,labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">def plot_with_labels(low_dim_embs,labels,<span class="attr">filename='tsne_skip.png'):</span></span><br><span class="line">    <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>]&gt;=len(labels),'More labels than embeddings'</span><br><span class="line">    plt.figure(<span class="attr">figsize=(18,18))</span> <span class="comment">#in inches</span></span><br><span class="line">    for i,label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        x,<span class="attr">y=low_dim_embs[i,:]</span></span><br><span class="line">        plt.scatter(x,y)</span><br><span class="line">        plt.annotate(label,</span><br><span class="line">                     <span class="attr">xy=(x,y),</span></span><br><span class="line">                     <span class="attr">xytext=(5,2),</span></span><br><span class="line">                     <span class="attr">textcoords='offset</span> points',</span><br><span class="line">                     <span class="attr">ha='right',</span></span><br><span class="line">                     <span class="attr">va='bottom')</span></span><br><span class="line">        plt.savefig(filename)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">__name__=="__main__":</span></span><br><span class="line">    <span class="comment">#filename=maybe_download('text8.zip',31344016)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">filename='text8.zip'</span></span><br><span class="line">    <span class="comment"># 获得词汇表</span></span><br><span class="line">    <span class="attr">vocabulary=read_data(filename)</span></span><br><span class="line">    print('Data size:',len(vocabulary))</span><br><span class="line">    <span class="comment"># 词表总长度:17005207</span></span><br><span class="line">    print(vocabulary[:<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># next,制作一个词表,将不常见的词变成一个UNK标识符</span></span><br><span class="line">    <span class="comment"># 词表的大小为5w,只考虑常出现的5w个词</span></span><br><span class="line">    <span class="attr">vocabulary_size=50000</span></span><br><span class="line">    data,count,dictionary,<span class="attr">reverse_dictionary=build_data(vocabulary,vocabulary_size)</span></span><br><span class="line">    del vocabulary <span class="comment"># 删除以节省内存</span></span><br><span class="line">    <span class="comment"># 输出最常见的5个单词</span></span><br><span class="line">    print('Most common words(+UNK)',count[:<span class="number">5</span>])</span><br><span class="line">    <span class="comment"># 输出转换后的数据库data和原来的单词(前10个)</span></span><br><span class="line">    print('Sample data:',data[:<span class="number">10</span>],[reverse_dictionary[i] for i <span class="keyword">in</span> data[:<span class="number">10</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 默认情况下skip_window=1,num_skips=2</span></span><br><span class="line">    <span class="comment"># 假如存在连续三个词['us','against','world']</span></span><br><span class="line">    <span class="comment"># 生成的两个样本就是: against-&gt;us,against-&gt;world</span></span><br><span class="line">    batch,<span class="attr">labels=generate_batch(batch_size=8,num_skips=2,skip_window=1)</span></span><br><span class="line">    for i <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">        print(batch[i],reverse_dictionary[batch[i]],</span><br><span class="line">              '-&gt;',labels[i,<span class="number">0</span>],reverse_dictionary[labels[i,<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 建立训练模型</span></span><br><span class="line">    <span class="comment"># 设置参数</span></span><br><span class="line">    <span class="attr">batch_size</span> = <span class="number">128</span></span><br><span class="line">    <span class="attr">embedding_size</span> = <span class="number">128</span>  <span class="comment"># 词嵌入空间是128维的</span></span><br><span class="line">    <span class="attr">skip_window</span> = <span class="number">1</span>  <span class="comment"># 与先前参数保持一致</span></span><br><span class="line">    <span class="attr">num_skips</span> = <span class="number">2</span>  <span class="comment"># 与先前参数保持一致</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在训练过程中,会对模型进行验证</span></span><br><span class="line">    <span class="comment"># 验证方法就是找出和某个词最相近的词</span></span><br><span class="line">    <span class="comment"># 只对valid_window的词进行验证,因为这些词最常出现</span></span><br><span class="line">    <span class="attr">valid_size</span> = <span class="number">16</span>  <span class="comment"># 每次验证16个词</span></span><br><span class="line">    <span class="attr">valid_window</span> = <span class="number">100</span>  <span class="comment"># 这16个词是在前100个最常见的词中选出来的</span></span><br><span class="line">    <span class="attr">valid_examples</span> = np.random.choice(valid_window, valid_size, <span class="attr">replace=False)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造损失时选取的噪声词的数量</span></span><br><span class="line">    <span class="attr">num_sampled</span> = <span class="number">64</span></span><br><span class="line">    <span class="attr">graph</span> = tf.Graph()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> graph.as_default():</span><br><span class="line">        <span class="comment"># 输入的batch</span></span><br><span class="line">        <span class="attr">train_inputs=tf.placeholder(tf.int32,shape=[batch_size])</span></span><br><span class="line">        <span class="attr">train_labels=tf.placeholder(tf.int32,shape=[batch_size,1])</span></span><br><span class="line">        <span class="comment"># 用于验证的词</span></span><br><span class="line">        <span class="attr">valid_dataset=tf.constant(valid_examples,dtype=tf.int32)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用cpu运行</span></span><br><span class="line">        <span class="keyword">with</span> tf.device('/cpu:<span class="number">0</span>'):</span><br><span class="line">            <span class="comment"># 定义1个embeddings变量,相当于一行存储一个词的embedding</span></span><br><span class="line">            <span class="attr">embeddings=tf.Variable(</span></span><br><span class="line">                tf.random_uniform([vocabulary_size,embedding_size],-<span class="number">1.0</span>,<span class="number">1.0</span>))</span><br><span class="line">            <span class="comment"># 利用embedding_lookup可以得到一个batch内的所有词嵌入</span></span><br><span class="line">            <span class="attr">embed=tf.nn.embedding_lookup(embeddings,train_inputs)</span></span><br><span class="line">            <span class="comment"># 在已经给定的词向量中查找索引</span></span><br><span class="line">            <span class="comment"># 创建两个变量用于NCE Loss(即选取噪声词的二分类损失)</span></span><br><span class="line">            <span class="attr">nce_weights=tf.Variable(</span></span><br><span class="line">                tf.truncated_normal([vocabulary_size,embedding_size],</span><br><span class="line">                                    <span class="attr">stddev=1.0/math.sqrt(embedding_size)))</span></span><br><span class="line">            <span class="attr">nce_biases=tf.Variable(tf.zeros([vocabulary_size]))</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># tf.nn.nce_loss会自动选取噪声词,并且形成损失</span></span><br><span class="line">            <span class="comment"># 随机选取num_sampled个噪声词</span></span><br><span class="line">            <span class="attr">loss=tf.reduce_mean(</span></span><br><span class="line">                tf.nn.nce_loss(<span class="attr">weights=nce_weights,</span></span><br><span class="line">                               <span class="attr">biases=nce_biases,</span></span><br><span class="line">                               <span class="attr">labels=train_labels,</span></span><br><span class="line">                               <span class="attr">inputs=embed,</span></span><br><span class="line">                               <span class="attr">num_sampled=num_sampled,</span></span><br><span class="line">                               <span class="attr">num_classes=vocabulary_size))</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 得到loss后,构造优化器</span></span><br><span class="line">            <span class="attr">optimizer=tf.train.GradientDescentOptimizer(0.01).minimize(loss)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算词和词的相似度(用于验证)</span></span><br><span class="line">            <span class="attr">norm=tf.sqrt(tf.reduce_mean(tf.square(embeddings),1,keep_dims=True))</span></span><br><span class="line">            <span class="attr">normalized_embeddings=embeddings/norm</span></span><br><span class="line">            <span class="comment"># 找出和验证词的embedding并计算它们和所有单词的相似度</span></span><br><span class="line">            <span class="attr">valid_embeddings=tf.nn.embedding_lookup(</span></span><br><span class="line">                normalized_embeddings,valid_dataset)</span><br><span class="line">            <span class="attr">similarity=tf.matmul(</span></span><br><span class="line">                valid_embeddings,normalized_embeddings,<span class="attr">transpose_b=True)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 变量初始化</span></span><br><span class="line">            <span class="attr">init=tf.global_variables_initializer()</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 开始训练</span></span><br><span class="line">            <span class="attr">num_steps=100001</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.Session(<span class="attr">graph=graph)</span> as session:</span><br><span class="line">                <span class="comment"># 初始化变量</span></span><br><span class="line">                init.run()</span><br><span class="line">                print('Initialized')</span><br><span class="line"></span><br><span class="line">                <span class="attr">average_loss=0</span></span><br><span class="line">                for step <span class="keyword">in</span> xrange(num_steps):</span><br><span class="line">                    batch_inputs,<span class="attr">batch_labels=generate_batch(</span></span><br><span class="line">                        batch_size,num_skips,skip_window)</span><br><span class="line">                    <span class="attr">feed_dict=&#123;train_inputs:batch_inputs,train_labels:batch_labels&#125;</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 优化一步</span></span><br><span class="line">                    _,<span class="attr">loss_val=session.run([optimizer,loss],feed_dict=feed_dict)</span></span><br><span class="line">                    average_loss+=loss_val</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 每2000次训练步骤显示一次训练结果</span></span><br><span class="line">                    <span class="keyword">if</span> step%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">if</span> step&gt;<span class="number">0</span>:</span><br><span class="line">                            average_loss/=<span class="number">2000</span></span><br><span class="line">                        <span class="comment"># 2000个batch的平均损失</span></span><br><span class="line">                        print('Average loss at step',step,':',average_loss)</span><br><span class="line">                        <span class="attr">average_loss=0</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 每10000次训练验证一次</span></span><br><span class="line">                    <span class="keyword">if</span> step%<span class="number">10000</span>==<span class="number">0</span>:</span><br><span class="line">                        <span class="comment"># sim:验证词与所有词之间的相似度</span></span><br><span class="line">                        <span class="attr">sim=similarity.eval()</span></span><br><span class="line">                        <span class="comment"># 一共有valid_size个验证词</span></span><br><span class="line">                        for i <span class="keyword">in</span> xrange(valid_size):</span><br><span class="line">                            <span class="attr">valid_word=reverse_dictionary[valid_examples[i]]</span></span><br><span class="line">                            <span class="attr">top_k=8</span> <span class="comment"># 输出最相邻的8个词语</span></span><br><span class="line">                            <span class="attr">nearest=(-sim[i,:]).argsort()[1:top_k+1]#</span> 这行代码含义不清晰</span><br><span class="line">                            <span class="attr">log_str='Nearest</span> to %s:'%valid_word</span><br><span class="line">                            for k <span class="keyword">in</span> xrange(top_k):</span><br><span class="line">                                <span class="attr">close_word=reverse_dictionary[nearest[k]]</span></span><br><span class="line">                                <span class="attr">log_str='%s</span> %s,'%(log_str,close_word)</span><br><span class="line">                            print(log_str)</span><br><span class="line">                <span class="comment"># final_embeddings是最后得到的embedding向量</span></span><br><span class="line">                <span class="comment"># 它的形状是[vocabulary_size,embedding_size]</span></span><br><span class="line">                <span class="comment"># 每一行就代表着对应index词的词嵌入表示</span></span><br><span class="line">                <span class="attr">final_embeddings=normalized_embeddings.eval()</span></span><br><span class="line">    <span class="comment"># 用t-SNE方法进行降维</span></span><br><span class="line">    <span class="attr">tsne</span> = TSNE(<span class="attr">perplexity=30,</span> <span class="attr">n_components=2,</span> <span class="attr">init='pca',</span> <span class="attr">n_iter=5000)</span></span><br><span class="line">    <span class="comment"># 只画出500个词的位置</span></span><br><span class="line">    <span class="attr">plot_only</span> = <span class="number">500</span></span><br><span class="line">    <span class="attr">low_dim_embs</span> = tsne.fit_transform(final_embeddings[:plot_only, :])</span><br><span class="line">    <span class="attr">labels</span> = [reverse_dictionary[i] for i <span class="keyword">in</span> xrange(plot_only)]</span><br><span class="line">    plot_with_labels(low_dim_embs, labels)</span><br></pre></td></tr></table></figure></p>
<p>其中进行了一个相似度生成，得到的图像如下：<br><img src="https://s2.ax1x.com/2019/10/18/KVHLaq.png" alt></p>
<h2 id="CBOW模型实现"><a href="#CBOW模型实现" class="headerlink" title="CBOW模型实现"></a>CBOW模型实现</h2><p>CBOW模型是通过上下文词来推导中心词，结构如下：<br><img src="https://s2.ax1x.com/2019/10/18/KVHzzF.png" alt></p>
<p>CBOW模型代码实现如下：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line">from __future__ <span class="built_in">import</span> absolute_import</span><br><span class="line">from __future__ <span class="built_in">import</span> division</span><br><span class="line">from __future__ <span class="built_in">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> collections</span><br><span class="line"><span class="built_in">import</span> math</span><br><span class="line"><span class="built_in">import</span> os</span><br><span class="line"><span class="built_in">import</span> random</span><br><span class="line"><span class="built_in">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line">from six.moves <span class="built_in">import</span> urllib</span><br><span class="line">from six.moves <span class="built_in">import</span> xrange</span><br><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"><span class="built_in">import</span> matplotlib</span><br><span class="line">matplotlib.use('agg')</span><br><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line">from sklearn.manifold <span class="built_in">import</span> TSNE</span><br><span class="line"></span><br><span class="line"><span class="attr">url='http://mattmahoney.net/dc/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件下载模块</span></span><br><span class="line">def maybe_download(filename,expected_bytes):</span><br><span class="line">    <span class="keyword">if</span> not os.path.exists(filename):</span><br><span class="line">        print('start downloading...')</span><br><span class="line">        filename,<span class="attr">_=urllib.request.urlretrieve(url+filename,filename)</span></span><br><span class="line">    <span class="attr">statinfo=os.stat(filename)</span></span><br><span class="line">    <span class="keyword">if</span> statinfo.<span class="attr">st_size==expected_bytes:</span></span><br><span class="line">        print('Found <span class="literal">and</span> verified',filename)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(statinfo.st_size)</span><br><span class="line">        raise Exception('Failed to verify ' + filename + '. Can you get to it <span class="keyword">with</span> a browser?')</span><br><span class="line">    return filename</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压语料库并转换成一个word的list</span></span><br><span class="line">def read_data(filename):</span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(filename) as f:</span><br><span class="line">        <span class="attr">data=tf.compat.as_str(f.read(f.namelist()[0])).split()</span></span><br><span class="line">        <span class="comment"># tf.compat.as_str 将数据转为单词列表</span></span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原始的单词表示为index</span></span><br><span class="line">def build_data(words,n_words):</span><br><span class="line">    <span class="attr">count=[['UNK',-1]]</span></span><br><span class="line">    count.extend(collections.Counter(words).most_common(n_words-<span class="number">1</span>))</span><br><span class="line">    <span class="attr">dictionary=dict()</span></span><br><span class="line">    <span class="comment"># 生成单词及编号</span></span><br><span class="line">    <span class="comment"># 单词顺序是从出现次数最多的单词到出现次数较小的单词，逐次递减的</span></span><br><span class="line">    for word,_ <span class="keyword">in</span> count:</span><br><span class="line">        dictionary[word]=len(dictionary)</span><br><span class="line">    <span class="attr">data=list()</span></span><br><span class="line">    <span class="attr">unk_count=0</span></span><br><span class="line">    for word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">            <span class="attr">index=dictionary[word]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="attr">index=0</span> <span class="comment"># UNK的index值设置为0</span></span><br><span class="line">            unk_count+=<span class="number">1</span></span><br><span class="line">        <span class="comment"># data表是由单词出现的次数大小生成的</span></span><br><span class="line">        data.append(index)</span><br><span class="line">    <span class="comment"># print(unk_count)</span></span><br><span class="line">    count[<span class="number">0</span>][<span class="number">1</span>]=unk_count</span><br><span class="line">    <span class="attr">reversed_dictionary=dict(zip(dictionary.values(),dictionary.keys()))</span></span><br><span class="line">    <span class="comment"># data: 所有单词的编号集合</span></span><br><span class="line">    <span class="comment"># count: 每个单词及其出现的次数</span></span><br><span class="line">    <span class="comment"># dictionary: 词典，包含了出现最多的5w个单词中的单词和对应的编号</span></span><br><span class="line">    <span class="comment"># reversed_dictionary: 反向词典</span></span><br><span class="line">    return data,count,dictionary,reversed_dictionary</span><br><span class="line"></span><br><span class="line"><span class="attr">data_index=0</span></span><br><span class="line"><span class="comment"># 用于cbow模型的batch生成器</span></span><br><span class="line">def generate_batch(batch_size,cbow_window):</span><br><span class="line">    global data_index</span><br><span class="line">    <span class="keyword">assert</span> cbow_window%<span class="number">2</span>==<span class="number">1</span></span><br><span class="line">    <span class="attr">span=2*cbow_window+1</span></span><br><span class="line">    <span class="comment"># 去除中心词:span-1</span></span><br><span class="line">    <span class="attr">batch=np.ndarray(shape=(batch_size,span-1),dtype=np.int32)</span></span><br><span class="line">    <span class="attr">labels=np.ndarray(shape=(batch_size,1),dtype=np.int32)</span></span><br><span class="line">    <span class="attr">buffer=collections.deque(maxlen=span)</span></span><br><span class="line">    for _ <span class="keyword">in</span> range(span):</span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">        <span class="comment"># 循环选取data中数据,到尾部则从头开始</span></span><br><span class="line">        <span class="attr">data_index=(data_index+1)%len(data)</span></span><br><span class="line"></span><br><span class="line">    for i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        <span class="comment"># 目标词在span的中心位置</span></span><br><span class="line">        <span class="attr">target=cbow_window</span></span><br><span class="line">        <span class="comment"># 仅仅需要知道上下文词而不需要中心词</span></span><br><span class="line">        <span class="attr">target_to_avoid=[cbow_window]</span></span><br><span class="line">        <span class="attr">col_idx=0</span></span><br><span class="line">        for j <span class="keyword">in</span> range(span):</span><br><span class="line">            <span class="comment"># 忽略中心词</span></span><br><span class="line">            <span class="keyword">if</span> <span class="attr">j</span> ==span//<span class="number">2</span>:</span><br><span class="line">                continue</span><br><span class="line">            batch[i,col_idx]=buffer[j]</span><br><span class="line">            col_idx+=<span class="number">1</span></span><br><span class="line">        labels[i,<span class="number">0</span>]=buffer[target]</span><br><span class="line">        <span class="comment"># 更新buffer</span></span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">        <span class="attr">data_index=(data_index+1)%len(data)</span></span><br><span class="line">    return batch,labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">def plot_with_labels(low_dim_embs, labels, <span class="attr">filename='tsne_cbow.png'):</span></span><br><span class="line">  <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), 'More labels than embeddings'</span><br><span class="line">  plt.figure(<span class="attr">figsize=(18,</span> <span class="number">18</span>))  <span class="comment"># in inches</span></span><br><span class="line">  for i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">    x, <span class="attr">y</span> = low_dim_embs[i, :]</span><br><span class="line">    plt.scatter(x, y)</span><br><span class="line">    plt.annotate(label,</span><br><span class="line">                 <span class="attr">xy=(x,</span> y),</span><br><span class="line">                 <span class="attr">xytext=(5,</span> <span class="number">2</span>),</span><br><span class="line">                 <span class="attr">textcoords='offset</span> points',</span><br><span class="line">                 <span class="attr">ha='right',</span></span><br><span class="line">                 <span class="attr">va='bottom')</span></span><br><span class="line"></span><br><span class="line">  plt.savefig(filename)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">__name__=="__main__":</span></span><br><span class="line">    <span class="attr">filename</span> = 'text8.zip'</span><br><span class="line">    <span class="comment"># 获得词汇表</span></span><br><span class="line">    <span class="attr">vocabulary</span> = read_data(filename)</span><br><span class="line">    <span class="attr">vocabulary_size</span> = <span class="number">50000</span></span><br><span class="line">    data, count, dictionary, <span class="attr">reverse_dictionary</span> = build_data(vocabulary, vocabulary_size)</span><br><span class="line">    del vocabulary  <span class="comment"># 删除以节省内存</span></span><br><span class="line">    <span class="comment"># 建立训练模型</span></span><br><span class="line">    <span class="comment"># 设置参数</span></span><br><span class="line">    <span class="attr">batch_size</span> = <span class="number">128</span></span><br><span class="line">    <span class="attr">embedding_size</span> = <span class="number">128</span>  <span class="comment"># 词嵌入空间是128维的</span></span><br><span class="line">    <span class="attr">cbow_window</span> = <span class="number">1</span>  <span class="comment"># 与先前参数保持一致</span></span><br><span class="line">    <span class="attr">num_skips</span> = <span class="number">2</span>  <span class="comment"># 与先前参数保持一致</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在训练过程中,会对模型进行验证</span></span><br><span class="line">    <span class="comment"># 验证方法就是找出和某个词最相近的词</span></span><br><span class="line">    <span class="comment"># 只对valid_window的词进行验证,因为这些词最常出现</span></span><br><span class="line">    <span class="attr">valid_size</span> = <span class="number">16</span>  <span class="comment"># 每次验证16个词</span></span><br><span class="line">    <span class="attr">valid_window</span> = <span class="number">100</span>  <span class="comment"># 这16个词是在前100个最常见的词中选出来的</span></span><br><span class="line">    <span class="attr">valid_examples</span> = np.array(random.sample(range(valid_window), valid_size // <span class="number">2</span>))</span><br><span class="line">    <span class="attr">valid_examples</span> = np.append(valid_examples, random.sample(range(<span class="number">1000</span>, <span class="number">1000</span> + valid_window), valid_size // <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造损失时选取的噪声词的数量</span></span><br><span class="line">    <span class="attr">num_sampled</span> = <span class="number">64</span></span><br><span class="line">    <span class="attr">graph</span> = tf.Graph()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练步数</span></span><br><span class="line">    <span class="attr">num_steps=100001</span></span><br><span class="line">    <span class="keyword">with</span> graph.as_default(),tf.device('/cpu:<span class="number">0</span>'):</span><br><span class="line">        <span class="comment"># 输入数据</span></span><br><span class="line">        <span class="attr">train_dataset=tf.placeholder(tf.int32,shape=[batch_size,2*cbow_window])</span></span><br><span class="line">        <span class="attr">train_labels=tf.placeholder(tf.int32,shape=[batch_size,1])</span></span><br><span class="line">        <span class="attr">valid_dataset=tf.constant(valid_examples,dtype=tf.int32)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 变量</span></span><br><span class="line">        <span class="comment"># embedding:词表中每个词的向量</span></span><br><span class="line">        <span class="attr">embeddings=tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))</span></span><br><span class="line">        <span class="attr">nce_weights=tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],</span></span><br><span class="line">                                                    <span class="attr">stddev=1.0/math.sqrt(embedding_size)))</span></span><br><span class="line">        <span class="attr">nce_biases=tf.Variable(tf.zeros([vocabulary_size]))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型</span></span><br><span class="line">        <span class="attr">embeds=None</span></span><br><span class="line">        for i <span class="keyword">in</span> range(<span class="number">2</span>*cbow_window):</span><br><span class="line">            <span class="attr">embedding_i=tf.nn.embedding_lookup(embeddings,train_dataset[:,i])</span></span><br><span class="line">            print('embedding %d shape: %s'%(i,embedding_i.get_shape().as_list()))</span><br><span class="line">            emb_x,<span class="attr">emb_y=embedding_i.get_shape().as_list()</span></span><br><span class="line">            <span class="keyword">if</span> embeds is None:</span><br><span class="line">                <span class="attr">embeds=tf.reshape(embedding_i,[emb_x,emb_y,1])</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="attr">embeds=tf.concat([embeds,tf.reshape(embedding_i,[emb_x,emb_y,1])],2)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> embeds.get_shape().as_list()[<span class="number">2</span>]==<span class="number">2</span>*cbow_window</span><br><span class="line">        print(<span class="string">"Concat embedding size: %s"</span>%embeds.get_shape().as_list())</span><br><span class="line">        <span class="attr">avg_embed=tf.reduce_mean(embeds,2,keep_dims=False)</span></span><br><span class="line">        print(<span class="string">"Avg embedding size: %s"</span>%avg_embed.get_shape().as_list())</span><br><span class="line"></span><br><span class="line">        <span class="attr">loss</span> = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases,</span><br><span class="line">                                             <span class="attr">labels=train_labels,</span></span><br><span class="line">                                             <span class="attr">inputs=avg_embed,</span></span><br><span class="line">                                             <span class="attr">num_sampled=num_sampled,</span></span><br><span class="line">                                             <span class="attr">num_classes=vocabulary_size))</span></span><br><span class="line">        <span class="attr">optimizer</span> = tf.train.AdagradOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line">        <span class="attr">norm</span> = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, <span class="attr">keep_dims=True))</span></span><br><span class="line">        <span class="attr">normalized_embeddings</span> = embeddings / norm</span><br><span class="line">        <span class="attr">valid_embeddings</span> = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)</span><br><span class="line">        <span class="attr">similarity</span> = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session(<span class="attr">graph=graph)</span> as session:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        print('Initialized')</span><br><span class="line">        <span class="attr">average_loss</span> = <span class="number">0</span></span><br><span class="line">        for step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">            batch_data, <span class="attr">batch_labels</span> = generate_batch(batch_size, cbow_window)</span><br><span class="line">            <span class="attr">feed_dict</span> = &#123;train_dataset: batch_data, train_labels: batch_labels&#125;</span><br><span class="line">            _, <span class="attr">l</span> = session.run([optimizer, loss], <span class="attr">feed_dict=feed_dict)</span></span><br><span class="line">            average_loss += l</span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="attr">average_loss</span> = average_loss / <span class="number">2000</span></span><br><span class="line">                    <span class="comment"># The average loss is an estimate of the loss over the last 2000 batches.</span></span><br><span class="line">                print('Average loss at step %d: %f' % (step, average_loss))</span><br><span class="line">                <span class="attr">average_loss</span> = <span class="number">0</span></span><br><span class="line">            <span class="comment"># note that this is expensive (~20% slowdown if computed every 500 steps)</span></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="attr">sim</span> = similarity.eval()</span><br><span class="line">                for i <span class="keyword">in</span> range(valid_size):</span><br><span class="line">                    <span class="attr">valid_word</span> = reverse_dictionary[valid_examples[i]]</span><br><span class="line">                    <span class="attr">top_k</span> = <span class="number">8</span>  <span class="comment"># number of nearest neighbors</span></span><br><span class="line">                    <span class="attr">nearest</span> = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k + <span class="number">1</span>]</span><br><span class="line">                    <span class="attr">log</span> = 'Nearest to %s:' % valid_word</span><br><span class="line">                    for k <span class="keyword">in</span> range(top_k):</span><br><span class="line">                        <span class="attr">close_word</span> = reverse_dictionary[nearest[k]]</span><br><span class="line">                        <span class="attr">log</span> = '%s %s,' % (log, close_word)</span><br><span class="line">                    print(log)</span><br><span class="line">        <span class="attr">final_embeddings</span> = normalized_embeddings.eval()</span><br><span class="line">    <span class="attr">tsne</span> = TSNE(<span class="attr">perplexity=30,</span> <span class="attr">n_components=2,</span> <span class="attr">init='pca',</span> <span class="attr">n_iter=5000)</span></span><br><span class="line">    <span class="comment"># 只画出500个词的位置</span></span><br><span class="line">    <span class="attr">plot_only</span> = <span class="number">500</span></span><br><span class="line">    <span class="attr">low_dim_embs</span> = tsne.fit_transform(final_embeddings[:plot_only, :])</span><br><span class="line">    <span class="attr">labels</span> = [reverse_dictionary[i] for i <span class="keyword">in</span> xrange(plot_only)]</span><br><span class="line">    plot_with_labels(low_dim_embs, labels)</span><br></pre></td></tr></table></figure></p>
<p>根据CBOW模型生成出来的图像如下所示：<br><img src="https://s2.ax1x.com/2019/10/18/KVbkIx.png" alt></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/18/Skip-Gram模型与CBOW模型代码实现/" data-id="ckymbgwq1001s0guk7oha50oa" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/10/23/从零实现基于医疗知识图谱的问答系统-一/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          从零实现基于医疗知识图谱的问答系统(一)-数据收集
        
      </div>
    </a>
  
  
    <a href="/2019/10/14/《统计学习方法》第四章-朴素贝叶斯法/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">《统计学习方法》第四章 朴素贝叶斯法</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
    <h3 class="widget-title">联系方式</h3>
    <div class="widget">
	  <li><a>klausvon@163.com</a></li>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">目录</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CTF/">CTF</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/HTML/">HTML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP-知识图谱/">NLP,知识图谱</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/VQA/">VQA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/XSS漏洞学习/">XSS漏洞学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/代码审计/">代码审计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/开发/">开发</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学学习/">数学学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/漏洞学习/">漏洞学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/环境配置/">环境配置</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/">神经网络</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CTF-Web/">CTF_Web</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HTML/">HTML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JavaScript/">JavaScript</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Model-Compression/">Model Compression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PHP/">PHP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL-injection/">SQL_injection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VQA/">VQA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XXE/">XXE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/凸包算法/">凸包算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/开发/">开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数学学习/">数学学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据挖掘/">数据挖掘</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/环境配置/">环境配置</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/知识图谱/">知识图谱</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 15px;">Algorithm</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CTF-Web/" style="font-size: 18.33px;">CTF_Web</a> <a href="/tags/HTML/" style="font-size: 10px;">HTML</a> <a href="/tags/JavaScript/" style="font-size: 10px;">JavaScript</a> <a href="/tags/ML/" style="font-size: 11.67px;">ML</a> <a href="/tags/Model-Compression/" style="font-size: 10px;">Model Compression</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/PHP/" style="font-size: 10px;">PHP</a> <a href="/tags/Python/" style="font-size: 11.67px;">Python</a> <a href="/tags/SQL-injection/" style="font-size: 13.33px;">SQL_injection</a> <a href="/tags/VQA/" style="font-size: 10px;">VQA</a> <a href="/tags/XXE/" style="font-size: 10px;">XXE</a> <a href="/tags/凸包算法/" style="font-size: 10px;">凸包算法</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a> <a href="/tags/开发/" style="font-size: 18.33px;">开发</a> <a href="/tags/数学学习/" style="font-size: 11.67px;">数学学习</a> <a href="/tags/数据挖掘/" style="font-size: 11.67px;">数据挖掘</a> <a href="/tags/环境配置/" style="font-size: 10px;">环境配置</a> <a href="/tags/知识图谱/" style="font-size: 16.67px;">知识图谱</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/01/12/单调栈的奇妙冒险/">单调栈的奇妙冒险</a>
          </li>
        
          <li>
            <a href="/2021/06/03/Basic-Understanding-of-Optimization/">Basic Understanding of Optimization</a>
          </li>
        
          <li>
            <a href="/2021/06/02/Why-Deep-Structure/">Why Deep Structure</a>
          </li>
        
          <li>
            <a href="/2021/05/23/Model-Compression-基本概念/">What is Model Compression</a>
          </li>
        
          <li>
            <a href="/2020/05/21/CSS在开发中的基本操作小结/">CSS在开发中的基本操作小结</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
	 <a class="theme-link"  href="https://mteacher.top/"> martini's blog </a><span>&nbsp;&nbsp;</span>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 Klaus<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>