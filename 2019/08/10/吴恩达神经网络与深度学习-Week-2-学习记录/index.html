<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>吴恩达神经网络与深度学习 Week-2 学习记录 | Klaus&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="写在最前，Week-1的内容我认为相对简单，看得比较粗略，所以没有做记录。其次，在学习了一段时间神经网络的NLP的应用之后，在搭建网络和相关基础知识上我存在很多欠缺，对论文中的一些细节把握得不够好，所以准备弥补一下各项短板来提升一下NLP方面的能力。吴恩达老师的课程讲得非常好，解决了我听cs224n课程中遇到的一些理解不了的问题，很有帮助，后续的博客中会记录吴老师神经网络与深度学习的所有课程记录。">
<meta name="keywords" content="神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达神经网络与深度学习 Week-2 学习记录">
<meta property="og:url" content="http://yoursite.com/2019/08/10/吴恩达神经网络与深度学习-Week-2-学习记录/index.html">
<meta property="og:site_name" content="Klaus&#39;s Blog">
<meta property="og:description" content="写在最前，Week-1的内容我认为相对简单，看得比较粗略，所以没有做记录。其次，在学习了一段时间神经网络的NLP的应用之后，在搭建网络和相关基础知识上我存在很多欠缺，对论文中的一些细节把握得不够好，所以准备弥补一下各项短板来提升一下NLP方面的能力。吴恩达老师的课程讲得非常好，解决了我听cs224n课程中遇到的一些理解不了的问题，很有帮助，后续的博客中会记录吴老师神经网络与深度学习的所有课程记录。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://i.loli.net/2019/08/10/iFm1ya9TKMUgqDu.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/Uzabtk5DJBpwrYv.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/wU7ZAg8qmGixySX.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/56XAoGHgJNTuPnl.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/9VgHBLziAuWMErI.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/JKL9EsqXjSidZPY.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/q5ZKiRtwc6HahBv.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/IfRK2XomzuJwgCA.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/iaGpTL8Pl56AnWq.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/CrmM97Ia1zs5tJh.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/8XxoQ3C1cV52Lpa.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/oFQUvmYa51EdPyr.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/PHwEgahoSK5BcRt.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/l7A5FxDMairUfIL.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/y7J3ZbEowPjl1xc.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/xKifm7GcEgPs3Wa.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/BFtDurpwls8Ra9o.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/tEp5yS2xsKFUm7N.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/D4vgSulRbicYAd6.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjyjJ.png">
<meta property="og:image" content="https://i.loli.net/2019/08/10/12GsfoHIJBdkC9a.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqX2Y8.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqX5Os.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqXomn.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjPk6.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjitK.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjApD.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjE1e.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjZXd.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjmnA.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqju7t.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjQtf.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjlh8.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjcu9.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjgBR.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjWAx.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjh4K.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqj59O.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjXUP.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjj4f.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqjz8S.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqvpvQ.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqvZCT.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqvm2F.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqv1V1.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqv856.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqv5aq.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqxpi6.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqx9JK.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqxeot.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqx1yQ.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/10/eqxJwn.png">
<meta property="og:updated_time" content="2019-08-11T18:12:58.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="吴恩达神经网络与深度学习 Week-2 学习记录">
<meta name="twitter:description" content="写在最前，Week-1的内容我认为相对简单，看得比较粗略，所以没有做记录。其次，在学习了一段时间神经网络的NLP的应用之后，在搭建网络和相关基础知识上我存在很多欠缺，对论文中的一些细节把握得不够好，所以准备弥补一下各项短板来提升一下NLP方面的能力。吴恩达老师的课程讲得非常好，解决了我听cs224n课程中遇到的一些理解不了的问题，很有帮助，后续的博客中会记录吴老师神经网络与深度学习的所有课程记录。">
<meta name="twitter:image" content="https://i.loli.net/2019/08/10/iFm1ya9TKMUgqDu.png">
  
    <link rel="alternate" href="/atom.xml" title="Klaus&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Klaus&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">MIA SAN MIA</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-吴恩达神经网络与深度学习-Week-2-学习记录" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/10/吴恩达神经网络与深度学习-Week-2-学习记录/" class="article-date">
  <time datetime="2019-08-09T18:03:57.000Z" itemprop="datePublished">2019-08-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/神经网络/">神经网络</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      吴恩达神经网络与深度学习 Week-2 学习记录
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  <!-- Table of Contents -->
		
		  <div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#二分分类"><span class="toc-number">1.</span> <span class="toc-text">二分分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-回归"><span class="toc-number">2.</span> <span class="toc-text">Logistic 回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic回归损失函数"><span class="toc-number">3.</span> <span class="toc-text">Logistic回归损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降法"><span class="toc-number">4.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#导数"><span class="toc-number">5.</span> <span class="toc-text">导数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#更多导数的例子"><span class="toc-number">6.</span> <span class="toc-text">更多导数的例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算图"><span class="toc-number">7.</span> <span class="toc-text">计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算图的导数计算"><span class="toc-number">8.</span> <span class="toc-text">计算图的导数计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic回归中的梯度下降算法"><span class="toc-number">9.</span> <span class="toc-text">Logistic回归中的梯度下降算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#m个样本梯度下降"><span class="toc-number">10.</span> <span class="toc-text">m个样本梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#向量化"><span class="toc-number">11.</span> <span class="toc-text">向量化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#向量化的更多例子"><span class="toc-number">12.</span> <span class="toc-text">向量化的更多例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#向量化Logistic回归"><span class="toc-number">13.</span> <span class="toc-text">向量化Logistic回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#向量化Logistic回归的梯度输出"><span class="toc-number">14.</span> <span class="toc-text">向量化Logistic回归的梯度输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python中的广播"><span class="toc-number">15.</span> <span class="toc-text">Python中的广播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#关于Python-numpy向量的说明"><span class="toc-number">16.</span> <span class="toc-text">关于Python\/numpy向量的说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现一个梯度下降算法"><span class="toc-number">17.</span> <span class="toc-text">实现一个梯度下降算法</span></a></li></ol>
		  </div>
		
        <p>写在最前，Week-1的内容我认为相对简单，看得比较粗略，所以没有做记录。<br>其次，在学习了一段时间神经网络的NLP的应用之后，在搭建网络和相关基础知识上我存在很多欠缺，对论文中的一些细节把握得不够好，所以准备弥补一下各项短板来提升一下NLP方面的能力。吴恩达老师的课程讲得非常好，解决了我听cs224n课程中遇到的一些理解不了的问题，很有帮助，后续的博客中会记录吴老师神经网络与深度学习的所有课程记录。</p>
<p>本博客中记录了该课程第二周的全部学习记录。</p>
<a id="more"></a>
<h2 id="二分分类"><a href="#二分分类" class="headerlink" title="二分分类"></a>二分分类</h2><p>假如给定一张图片，任何图片都由三原色组成，也就是通道数为3，假如将一张大小为64x64的图片输入神经网络，则输入矩阵维度是（64，64，3）<br><img src="https://i.loli.net/2019/08/10/iFm1ya9TKMUgqDu.png" alt><br>其中，三色通道完整的矩阵表示如下：<br><img src="https://i.loli.net/2019/08/10/Uzabtk5DJBpwrYv.png" alt><br>在二分类的任务中，用一个元组来进行表示 （x，y），x表示输入（即一个n_x维数据，为输入数据），y表示输出的结果（即用0 和 1 来表示图片中是否有猫）。<br><img src="https://i.loli.net/2019/08/10/wU7ZAg8qmGixySX.png" alt><br>其中，x是大小为Nx的矩阵，y作为标签只有0和1，m个样本组成下面m个元组的训练集。<br>将对应的训练集构成一个矩阵X输入神经网络进行训练，其中，训练样本数作为col相较于训练样本作为row要便于训练，X的大小为（Nx，m）<br><img src="https://i.loli.net/2019/08/10/56XAoGHgJNTuPnl.png" alt><br>同理，对于Y，为了方便训练，也将样本数作为col，如下图所示，大小为（1,m）：<br><img src="https://i.loli.net/2019/08/10/9VgHBLziAuWMErI.png" alt></p>
<h2 id="Logistic-回归"><a href="#Logistic-回归" class="headerlink" title="Logistic 回归"></a>Logistic 回归</h2><p>在二分类中，给定一张图片，需要识别出来是不是猫，即给定x，预测得到 y_hat，其中 y_hat 是一个概率，y_hat=p(y=1|x)，也就是，当输入特征 x 满足条件时，y就是1。<br><img src="https://i.loli.net/2019/08/10/JKL9EsqXjSidZPY.png" alt><br>在上述线性回归中一共有两个参数，w 和 b，通过output中的式子计算出来最后的 y_hat。<br>However，这不是一个好的线性回归算法，前提已经预设了y=1，所以y_hat应该介于0和1之间，但是这其中的值并不容易实现，它的值可能比1大得多，甚至是负值。<br>所以，可以在计算y_hat的时候加上一个sigmoid函数，sigmoid函数图像如下：<br><img src="https://i.loli.net/2019/08/10/q5ZKiRtwc6HahBv.png" alt></p>
<p>添加sigmoid函数后的ioutput函数结果：<br><img src="https://i.loli.net/2019/08/10/IfRK2XomzuJwgCA.png" alt></p>
<p>在做逻辑回归时，要做的就是学习参数w和b,所以，y_hat就会成为较好的估计。</p>
<p>在对神经网络进行编程时，通常会将w和参数b分开。</p>
<h2 id="Logistic回归损失函数"><a href="#Logistic回归损失函数" class="headerlink" title="Logistic回归损失函数"></a>Logistic回归损失函数</h2><p>为了计算2-2中提到的 w 和 b，需要定义一个代价函数（Cost Function）和一个损失函数（Cost Function）。<br>损失函数（Loss Function）的定义：<br>（因为这里是一个二分类任务，所以y的值只有0和1（个人理解））<br><img src="https://i.loli.net/2019/08/10/iaGpTL8Pl56AnWq.png" alt></p>
<p>对上述两个公式的详解，当y=1 or 0，要求L(y,y_hat)的值尽量接近 1 or 0。<br><img src="https://i.loli.net/2019/08/10/CrmM97Ia1zs5tJh.png" alt></p>
<p>其中第一种求平方的损失函数的 定义方法得到的结果是非凸的（如下），有多个最优解：<br><img src="https://i.loli.net/2019/08/10/8XxoQ3C1cV52Lpa.png" alt></p>
<p>第二种求解方式可以得到一个凸函数，有利于求出最优解：<br><img src="https://i.loli.net/2019/08/10/oFQUvmYa51EdPyr.png" alt></p>
<font color="red">所以，常用的损失函数计算方式为第二种。</font>

<p>Finally，损失函数（Loss Function）在单个训练样本中衡量了单个样本上的表现，反之，代价函数（Cost Function）衡量的是在全体样本上的表现。<br>代价函数（Cost Function）的定义：<br><img src="https://i.loli.net/2019/08/10/PHwEgahoSK5BcRt.png" alt></p>
<p>从形式上来看，代价函数有点类似于针对损失函数的每个样本值求和之后取平均。</p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>2-2中提到需要计算出时J(w,b)尽可能小的w和b，这就涉及到了使用梯度下降法。<br>以下图为例：<br><img src="https://i.loli.net/2019/08/10/l7A5FxDMairUfIL.png" alt></p>
<p>根据J(w,b)的定义，要在以w轴和b轴组成的界面上找到一个使得J(w,b)值最小的点，可以看出，成本函数J是一个凸函数。<br>为了找到最合适的w和b，要做的就是初始化w和b（上图中小红点所示的值），<font color="#0099ff">对于Logistic回归而言，几乎是任意的初始化方法都有效</font>，通常使用0来初始化。<br>梯度下降法所做的是，从初始点开始，朝最抖的下坡方向走一步，以最快的速度往下走，通过不断的迭代到达目的位置。<br><img src="https://i.loli.net/2019/08/10/y7J3ZbEowPjl1xc.png" alt><br>(绿色部分：学习率（Learning Rate），学习率可以控制每一次迭代或者梯度下降法中的步长)<br>上图所示为省略了b的梯度下降模拟，这里只需要更新w参数，这里要通过J(w)对w求导，重复这个更新操作。</p>
<p><img src="https://i.loli.net/2019/08/10/xKifm7GcEgPs3Wa.png" alt><br>上图右边部分的J(w)的导数值为正（斜率为正），乘以学习率，新的w值计算为：w=w-αw，导数是正的，从w中减去这个乘积，使得w不断减小，然后朝左边一步，不断循环这个过程，不断迭代，最终到达最低点。<br>上图左边部分的J(w)的导数值为负（斜率为负），乘以学习率，新的w值计算为：w=w-αw，不过这里的w是负数，慢慢地使得参数w增加，再往右边移动，循环这个过程来达到最低点。</p>
<h2 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h2><p>这个..,大概放一张图就行了，这节的内容很简单。<br><img src="https://i.loli.net/2019/08/10/BFtDurpwls8Ra9o.png" alt></p>
<h2 id="更多导数的例子"><a href="#更多导数的例子" class="headerlink" title="更多导数的例子"></a>更多导数的例子</h2><p>其他导数的栗子<br><img src="https://i.loli.net/2019/08/10/tEp5yS2xsKFUm7N.png" alt></p>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>计算图表示的是整个式子的计算流程，比较简单，如图所示：<br><img src="https://i.loli.net/2019/08/10/D4vgSulRbicYAd6.png" alt></p>
<p>从左到右的计算中可以得到J的最终值，反向箭头（红色）的计算详情见下节。<br><img src="https://s2.ax1x.com/2019/08/10/eqjyjJ.png" alt></p>
<h2 id="计算图的导数计算"><a href="#计算图的导数计算" class="headerlink" title="计算图的导数计算"></a>计算图的导数计算</h2><p>Q：如何利用计算图计算出J的导数？<br>假设存在如下一张流程图：<br><img src="https://i.loli.net/2019/08/10/12GsfoHIJBdkC9a.png" alt></p>
<p>需要J对v的导数，需要怎么计算？<br><img src="https://s2.ax1x.com/2019/08/10/eqX2Y8.png" alt></p>
<p>J=3v，J随v的改变而改变，完成dJ\/dv，也就完成了第一步反向求导。<br>下一步，求dJ\/da，变化的过程是：改变a→改变v→改变J，即链式求导法则， 所以：<br>dJ\/da=dJ\/dv * dv\/da= 3<br>完成流程示意图如下：<br><img src="https://s2.ax1x.com/2019/08/10/eqX5Os.png" alt></p>
<font color="red">注：当你想编程实现反向传播时，通常需要关心一个最终的输出值（中间底部红色部分标注）。</font>

<p>计算 dJ\/du=dJ\/dv * dv\/du=3 的流程图如下：<br><img src="https://s2.ax1x.com/2019/08/10/eqXomn.png" alt></p>
<p>同理，dJ\/db=dJ\/dv * dv\/du * du\/db=dJ\/du * du\/db<br>从上述计算中可知，dJ\/du=3<br>根据变量（或是直接求偏导）可得du\/db=2<br>所以，dJ\/db=6.</p>
<p>Finally，dJ\/dc * dJ\/du * du\/dc=9.<br>总结：一个计算流程图，从左往右计算代价函数（cost function）J，你可能需要优化的函数，然后反向从右到左计算导数。</p>
<h2 id="Logistic回归中的梯度下降算法"><a href="#Logistic回归中的梯度下降算法" class="headerlink" title="Logistic回归中的梯度下降算法"></a>Logistic回归中的梯度下降算法</h2><p>首先回顾一下逻辑回归（Logistic Regression）的公式，其中，a是逻辑回归计算后得到的输出结果，y是样本的基本真值标签值。<br><img src="https://s2.ax1x.com/2019/08/10/eqjPk6.png" alt></p>
<p>假设样本只有两个特征值 x1 和 x2，为了计算 z ，需要引入参数 w1 和 w2 和 b，通过这些值可以得到 z 的求值公式：z=w1x1+w2x2+b，然后计算y_hat=a=sigmoid(z)，最后计算L(a,y)=-（yloga+(1-y)log(1-a)），整个流程如下图所示：<br><img src="https://s2.ax1x.com/2019/08/10/eqjitK.png" alt></p>
<p>在逻辑回归中，我们需要做的是变换参数 w 和 b 的值来最小化损失函数。<br>上述描述是一个正向传播的过程，现在来讨论反向传播的过程。</p>
<p>想要计算损失函数L的导数，要先往前一步，计算dL/da，通过偏导计算，可得:<br>dL/da=-y/a + (1+y)/1-a<br>下一步计算dL/dz，这个步骤有点繁琐，所以写在纸上，利用的是链式法则，推导过程如下：<br><img src="https://s2.ax1x.com/2019/08/10/eqjApD.png" alt></p>
<p>吴恩达老师的推导示意图如下：<br><img src="https://s2.ax1x.com/2019/08/10/eqjE1e.png" alt></p>
<p>上述过程即一次梯度下降更新的结果，逻辑回归算法中不仅有一个样本，而是有m个样本的整个训练集。</p>
<h2 id="m个样本梯度下降"><a href="#m个样本梯度下降" class="headerlink" title="m个样本梯度下降"></a>m个样本梯度下降</h2><p>前面提到了单个样本的计算，现在需要对m个样本进行计算，回顾一边Cost Function定义：<br><img src="https://s2.ax1x.com/2019/08/10/eqjZXd.png" alt></p>
<p>其中，求导的过程是类似的：<br><img src="https://s2.ax1x.com/2019/08/10/eqjmnA.png" alt></p>
<p>求m个样本的梯度下降法，即先正向传播，再反向传播：<br>使用 dw1，dw1 和 db 作为累加器。<br><img src="https://s2.ax1x.com/2019/08/10/eqju7t.png" alt></p>
<p>当前示例只包含两个特征，假如存在多个特征，则必须dw3,dw4…这样一直计算下去，最后计算平均值，这样就完成了整个计算步骤。<br><img src="https://s2.ax1x.com/2019/08/10/eqjQtf.png" alt></p>
<p>后续需要对 w1,w2,b 等参数进行更新（α是学习率）：<br><img src="https://s2.ax1x.com/2019/08/10/eqjlh8.png" alt></p>
<font color="#0099ff">注：在编程实现过程中，假如存在多个特征，就必须从 dw1,dw2,…,dwn，遍历所有的n个特征，所以不使用显式for循环，可以有效提高使用效率。有一门向量化技术可以帮助解决这个问题，向量化可以用来加速运算。在深度学习时代，使用向量化来摆脱for循环已经变得非常重要。</font>


<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>向量是消除代码中显式for循环语句的艺术。<br>左半部分需要不断循环计算，而右边部分仅需要一个矩阵就能完成。<br><img src="https://s2.ax1x.com/2019/08/10/eqjcu9.png" alt></p>
<p>这里用一个小例子来说明一下向量化和循环之间计算耗时的差距，和吴恩达老师有点点差别，也就是Jupyter和Pycharm的区别，个人更习惯后者。<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line">#coding:utf-<span class="number">8</span></span><br><span class="line"></span><br><span class="line">import numpy as <span class="built_in">np</span></span><br><span class="line">import <span class="built_in">time</span></span><br><span class="line"></span><br><span class="line">'''</span><br><span class="line"># 生成一个<span class="built_in">np</span>类型的<span class="built_in">array</span></span><br><span class="line">a=<span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">'''</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    a=<span class="built_in">np</span>.<span class="built_in">random</span>.rand(<span class="number">1000000</span>)</span><br><span class="line">    b=<span class="built_in">np</span>.<span class="built_in">random</span>.rand(<span class="number">1000000</span>)</span><br><span class="line"></span><br><span class="line">    tic=<span class="built_in">time</span>.<span class="built_in">time</span>()</span><br><span class="line">    c=<span class="built_in">np</span>.dot(a,b)</span><br><span class="line">    toc=<span class="built_in">time</span>.<span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Vectorized version:"</span>,<span class="number">1000</span>*(toc-tic),'ms')</span><br><span class="line"></span><br><span class="line">    c=<span class="number">0</span></span><br><span class="line">    tic = <span class="built_in">time</span>.<span class="built_in">time</span>()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">        c+=a[i]*b[i]</span><br><span class="line">    toc = <span class="built_in">time</span>.<span class="built_in">time</span>()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"For Loop:"</span>, <span class="number">1000</span> * (toc - tic), 'ms')</span><br></pre></td></tr></table></figure></p>
<p>结果一目了然，毫无疑问向量化的匀速速度更快，快了接近300倍。<br><img src="https://s2.ax1x.com/2019/08/10/eqjgBR.png" alt></p>
<h2 id="向量化的更多例子"><a href="#向量化的更多例子" class="headerlink" title="向量化的更多例子"></a>向量化的更多例子</h2><p>经验法则：当构建法则或是做回归时，要尽量避免for循环。<br>举个栗子：如果你想计算一个向量u作为矩阵A和另一个向量v的乘积，矩阵乘法的定义就是u_i等于A_ij乘以v_j对j求和。<br><img src="https://s2.ax1x.com/2019/08/10/eqjWAx.png" alt></p>
<p>下图，左边部分使用循环计算，右边部分使用点乘。<br><img src="https://s2.ax1x.com/2019/08/10/eqjh4K.png" alt></p>
<p>可以看出，使用向量的版本，消除了两个循环。<br>在逻辑回归中使用向量化：<br><img src="https://s2.ax1x.com/2019/08/10/eqj59O.png" alt></p>
<h2 id="向量化Logistic回归"><a href="#向量化Logistic回归" class="headerlink" title="向量化Logistic回归"></a>向量化Logistic回归</h2><p>假如有m个样本，则需要对m个样本分别进行下面的运算（图示只到3，注：#盖里奇不会数到3，#全世界最不好的盖里奇）。<br><img src="https://s2.ax1x.com/2019/08/10/eqjXUP.png" alt></p>
<p>下面通过一个例子来展示如何使用向量将上述过程写在一个向量里：<br><img src="https://s2.ax1x.com/2019/08/10/eqjj4f.png" alt><br>需要转置的w可以是一个行向量（row vector）。</p>
<h2 id="向量化Logistic回归的梯度输出"><a href="#向量化Logistic回归的梯度输出" class="headerlink" title="向量化Logistic回归的梯度输出"></a>向量化Logistic回归的梯度输出</h2><p>这节的主要目的是学会使用向量来同时计算所有样本的下降梯度，为了避免忘掉前面的知识，这里补一张之前讲过的计算图（主要方式：链式求导）：<br><img src="https://s2.ax1x.com/2019/08/10/eqjz8S.png" alt></p>
<p>假如有m个样本需要计算，对m个训练样本做计算过程都是相同的：<br><img src="https://s2.ax1x.com/2019/08/10/eqvpvQ.png" alt></p>
<p>所以，仅需要一行代码，就能完成所有的计算，但是仍然需要一个for循环来计算各个dw和db的值：<br><img src="https://s2.ax1x.com/2019/08/10/eqvZCT.png" alt></p>
<p>接着，可以通过向量的方式再消掉基于样本的循环，一个循环修改为两行向量计算的代码即可，如下图所示：<br><img src="https://s2.ax1x.com/2019/08/10/eqvm2F.png" alt></p>
<p>下面给出完整的，只需要使用一个for循环的Logistic梯度下降描述，这个循环是必须的，因为要更新不同的dw和db来确定最终的w和b：<br><img src="https://s2.ax1x.com/2019/08/10/eqv1V1.png" alt></p>
<h2 id="Python中的广播"><a href="#Python中的广播" class="headerlink" title="Python中的广播"></a>Python中的广播</h2><p>下面的矩阵中列出了100g不同的食物中碳水化合物，蛋白质，脂肪的占比。<br><img src="https://s2.ax1x.com/2019/08/10/eqv856.png" alt></p>
<p>现在的目标是，计算四种食物中，各种含量占卡路里有多少百分比？<br>以Apple为例，卡路里=56+1.2+1.8=59（btw，I don’t like this number.）<br>∴碳水化合物的占比是：56\/59≈94.9%</p>
<p>Q：可以不使用for循环来计算各成分的占比吗？<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attribute">__name__</span>=="__main__":</span><br><span class="line">    <span class="attribute">a</span>=np.array([[56.0,0.0,4.4,68.0],[1.2,104.0,52.0,8.0],[1.8,135.0,99.0,0.9]],<span class="attribute">dtype</span>=np.float32)</span><br><span class="line">    <span class="attribute">col</span>=np.sum(a,axis=0)</span><br><span class="line">    # <span class="builtin-name">print</span>(col)</span><br><span class="line">    <span class="attribute">percentage</span>=100*a/col.reshape(1,4)</span><br><span class="line">    <span class="builtin-name">print</span>(percentage)</span><br></pre></td></tr></table></figure></p>
<p>运算结果如下：<br><img src="https://s2.ax1x.com/2019/08/10/eqv5aq.png" alt></p>
<p>Python 矩阵计算中其他栗子，numpy会根据计算矩阵的大小自动调整对应值。<br><img src="https://s2.ax1x.com/2019/08/10/eqxpi6.png" alt></p>
<p>具体运算规则如下：<br><img src="https://s2.ax1x.com/2019/08/10/eqx9JK.png" alt></p>
<h2 id="关于Python-numpy向量的说明"><a href="#关于Python-numpy向量的说明" class="headerlink" title="关于Python\/numpy向量的说明"></a>关于Python\/numpy向量的说明</h2><p>这一节中上半部分的内容主要看下面的代码即可：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 生成5个高斯变量</span></span><br><span class="line">    a=np.random.randn(<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 输出 a 的结果</span></span><br><span class="line">    print(a)</span><br><span class="line">    <span class="comment"># 输出 a 的形状</span></span><br><span class="line">    <span class="comment"># 注:这种生成的结果既不是行向量也不是列向量</span></span><br><span class="line">    print(a.shape)</span><br><span class="line">    <span class="comment"># 输出 a 的转置</span></span><br><span class="line">    print(a.T)</span><br><span class="line">    <span class="comment"># 输出 a 的点积</span></span><br><span class="line">    print(np.dot(a,a.T))</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    上述写法中，生成a的随机高斯变量方式不合适，无法确定一个向量，按照下面的方式写要更合适</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    a=np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">    print(a)</span><br><span class="line">    print(a.T)</span><br><span class="line">    print(np.dot(a,a.T))</span><br><span class="line">    <span class="comment"># 可以看出结果和先前的写法完全不同</span></span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><img src="https://s2.ax1x.com/2019/08/10/eqxeot.png" alt></p>
<p>后续吴老师讲到的一些小技巧：<br>当代码较多时，不确定Matrix的大小是否和预期一样，可以使用assert：<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">assert(<span class="name">a</span>.shape==(<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<h2 id="实现一个梯度下降算法"><a href="#实现一个梯度下降算法" class="headerlink" title="实现一个梯度下降算法"></a>实现一个梯度下降算法</h2><p>这里以Rosenbrock在1960年提出的山谷函数为例，Rosenbrock函数定义如下：<br><img src="https://s2.ax1x.com/2019/08/10/eqx1yQ.png" alt></p>
<p>函数 f 分别对 x,y 求导得到：<br><img src="https://s2.ax1x.com/2019/08/10/eqxJwn.png" alt></p>
<p>在实现的过程中可以给出x, y初始值(例如设置为 0, 0) 然后计算函数在这个点的梯度，并按照梯度方向更新x, y的值。<br>这里给出通过梯度下降法计算上述函数的最小值对应的x 和 y。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def cal_rosenbrock(x,y):</span><br><span class="line">    # 计算 Rosenbrock 的值</span><br><span class="line">    return pow((1-x),2)+100*pow((y-pow(x,2)),2)</span><br><span class="line"></span><br><span class="line">def cal_rosenbrock_praX(x,y):</span><br><span class="line">    # 对 x 求偏导</span><br><span class="line">    return -2*(1-x)-2<span class="number">*100</span>*(y-pow(x,2))<span class="number">*2</span>*x</span><br><span class="line"></span><br><span class="line">def cal_rosenbrock_praY(x,y):</span><br><span class="line">    # 对 y 求偏导</span><br><span class="line">    return 2<span class="number">*100</span>*(y-pow(x,2))</span><br><span class="line"></span><br><span class="line">def rosenbrock(<span class="attribute">max_iter_count</span>=100000,step_size=0.0001):</span><br><span class="line">    <span class="attribute">preX</span>=np.zeros((2,),dtype=np.float32)</span><br><span class="line">    # 初始值</span><br><span class="line">    <span class="attribute">loss</span>=10</span><br><span class="line">    <span class="attribute">iter_count</span>=0</span><br><span class="line">    <span class="keyword">while</span> loss&gt;0.001 <span class="keyword">and</span> iter_count&lt;max_iter_count:</span><br><span class="line">        <span class="attribute">error</span>=np.zeros((2,),dtype=np.float32)</span><br><span class="line">        <span class="builtin-name">error</span>[0]=cal_rosenbrock_praX(preX[0],preX[1])</span><br><span class="line">        <span class="builtin-name">error</span>[1]=cal_rosenbrock_praY(preX[0],preX[1])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(2):</span><br><span class="line">            preX[j]<span class="attribute">-</span>=step_size*error[j]</span><br><span class="line">            # 迭代计算</span><br><span class="line"></span><br><span class="line">        <span class="attribute">loss</span>=cal_rosenbrock(preX[0],preX[1])</span><br><span class="line"></span><br><span class="line">        <span class="builtin-name">print</span>(<span class="string">"iter count:"</span>,iter_count,<span class="string">"   the loss is :"</span>,loss)</span><br><span class="line">        iter_count+=1</span><br><span class="line"></span><br><span class="line">    return preX</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attribute">__name__</span>=="__main__":</span><br><span class="line">    <span class="attribute">w</span>=rosenbrock()</span><br><span class="line">    <span class="builtin-name">print</span>(w)</span><br></pre></td></tr></table></figure>
<p>参考博客：<a href="https://blog.csdn.net/pengjian444/article/details/71075544" target="_blank" rel="noopener">https://blog.csdn.net/pengjian444/article/details/71075544</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/10/吴恩达神经网络与深度学习-Week-2-学习记录/" data-id="ckymbgwrs00670gukz4oza4vn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/08/12/Logistic-Regression在图像识别的实现/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Logistic Regression在图像识别的实现
        
      </div>
    </a>
  
  
    <a href="/2019/08/08/cs224n-Lecture-2-词向量表示：word2vec/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs224n-Lecture-2 词向量表示：word2vec</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
    <h3 class="widget-title">联系方式</h3>
    <div class="widget">
	  <li><a>klausvon@163.com</a></li>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">目录</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CTF/">CTF</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/HTML/">HTML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP-知识图谱/">NLP,知识图谱</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/VQA/">VQA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/XSS漏洞学习/">XSS漏洞学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/代码审计/">代码审计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/开发/">开发</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学学习/">数学学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/漏洞学习/">漏洞学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/环境配置/">环境配置</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/">神经网络</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CTF-Web/">CTF_Web</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HTML/">HTML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JavaScript/">JavaScript</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Model-Compression/">Model Compression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PHP/">PHP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL-injection/">SQL_injection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VQA/">VQA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XXE/">XXE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/凸包算法/">凸包算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/开发/">开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数学学习/">数学学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据挖掘/">数据挖掘</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/环境配置/">环境配置</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/知识图谱/">知识图谱</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 15px;">Algorithm</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CTF-Web/" style="font-size: 18.33px;">CTF_Web</a> <a href="/tags/HTML/" style="font-size: 10px;">HTML</a> <a href="/tags/JavaScript/" style="font-size: 10px;">JavaScript</a> <a href="/tags/ML/" style="font-size: 11.67px;">ML</a> <a href="/tags/Model-Compression/" style="font-size: 10px;">Model Compression</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/PHP/" style="font-size: 10px;">PHP</a> <a href="/tags/Python/" style="font-size: 11.67px;">Python</a> <a href="/tags/SQL-injection/" style="font-size: 13.33px;">SQL_injection</a> <a href="/tags/VQA/" style="font-size: 10px;">VQA</a> <a href="/tags/XXE/" style="font-size: 10px;">XXE</a> <a href="/tags/凸包算法/" style="font-size: 10px;">凸包算法</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a> <a href="/tags/开发/" style="font-size: 18.33px;">开发</a> <a href="/tags/数学学习/" style="font-size: 11.67px;">数学学习</a> <a href="/tags/数据挖掘/" style="font-size: 11.67px;">数据挖掘</a> <a href="/tags/环境配置/" style="font-size: 10px;">环境配置</a> <a href="/tags/知识图谱/" style="font-size: 16.67px;">知识图谱</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/01/12/单调栈的奇妙冒险/">单调栈的奇妙冒险</a>
          </li>
        
          <li>
            <a href="/2021/06/03/Basic-Understanding-of-Optimization/">Basic Understanding of Optimization</a>
          </li>
        
          <li>
            <a href="/2021/06/02/Why-Deep-Structure/">Why Deep Structure</a>
          </li>
        
          <li>
            <a href="/2021/05/23/Model-Compression-基本概念/">What is Model Compression</a>
          </li>
        
          <li>
            <a href="/2020/05/21/CSS在开发中的基本操作小结/">CSS在开发中的基本操作小结</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
	 <a class="theme-link"  href="https://mteacher.top/"> martini's blog </a><span>&nbsp;&nbsp;</span>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 Klaus<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>