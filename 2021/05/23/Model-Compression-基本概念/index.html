<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>What is Model Compression | Klaus&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="course link: http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html PPT link: https://slides.com/arvinliu/model-compression/fullscreen">
<meta name="keywords" content="Model Compression">
<meta property="og:type" content="article">
<meta property="og:title" content="What is Model Compression">
<meta property="og:url" content="http://yoursite.com/2021/05/23/Model-Compression-基本概念/index.html">
<meta property="og:site_name" content="Klaus&#39;s Blog">
<meta property="og:description" content="course link: http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html PPT link: https://slides.com/arvinliu/model-compression/fullscreen">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://i.loli.net/2021/05/23/vXFEitOnq48TlYg.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/FGIC4XzPNubBgc3.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/8ah5wqeA4XGWnvT.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/SqbklAcxIRh67d4.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/mUXodnkbNyKaHLr.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/Gt3ideg5OZUFkER.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/TNA1wcrM9S4XPO2.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/5EGOWPacpUksY2j.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/sYwSP2qvriKVf1l.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/i8yJxVszpYjcG9g.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/IMczHJLQhd5P1Dr.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/jglL4I85nqp3YVS.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/K4nUwYSkWhZoceR.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/DuqPHK7gf6r1BOG.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/DFKlTIoxfn5CquS.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/ya2HRIsqZV3idmQ.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/3HkEhNT8R7m4KPb.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/52UlhIKdLcYnvqA.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/L1zb7FY46flaWsC.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/eCabWYMsZV1JD68.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/7zs2OSXUGuoTjQg.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/SXakzRMHjIsyofx.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/edxjPAawOFLKTvu.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/THC75AEKtvIjZMS.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/2c9JCWVKitg6LeQ.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/tdchXWmkCr9xsH5.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/AIfVgxznNWDTdvX.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/d2OV5Sga6TNeiqz.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/r1YvwBEpyzdcq3Z.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/T8pnQHjaCuWE4IG.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/nqktzL6Wur5BxpI.png">
<meta property="og:image" content="https://i.loli.net/2021/05/23/FodLXxGHzf7MQiS.png">
<meta property="og:updated_time" content="2021-06-02T15:26:54.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="What is Model Compression">
<meta name="twitter:description" content="course link: http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html PPT link: https://slides.com/arvinliu/model-compression/fullscreen">
<meta name="twitter:image" content="https://i.loli.net/2021/05/23/vXFEitOnq48TlYg.png">
  
    <link rel="alternate" href="/atom.xml" title="Klaus&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Klaus&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">MIA SAN MIA</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Model-Compression-基本概念" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/05/23/Model-Compression-基本概念/" class="article-date">
  <time datetime="2021-05-23T08:53:09.000Z" itemprop="datePublished">2021-05-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      What is Model Compression
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  <!-- Table of Contents -->
		
        <p>course link: <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html</a></p>
<p>PPT link: <a href="https://slides.com/arvinliu/model-compression/fullscreen" target="_blank" rel="noopener">https://slides.com/arvinliu/model-compression/fullscreen</a></p>
<a id="more"></a>
<h2 id="1-Knowledge-Distillation"><a href="#1-Knowledge-Distillation" class="headerlink" title="1. Knowledge Distillation"></a>1. Knowledge Distillation</h2><p>KD的主要问题：Distill what?</p>
<ul>
<li>logits (输出值)<ul>
<li>直接匹配 logits</li>
<li>学习一个batch里面的 logits distribution</li>
</ul>
</li>
<li>Feature (中间值)<ul>
<li>直接匹配中间的 Feature</li>
<li>学习 Feature 中间如何转换</li>
</ul>
</li>
</ul>
<h3 id="1-1-Logits-Distillation"><a href="#1-1-Logits-Distillation" class="headerlink" title="1.1. Logits Distillation"></a>1.1. Logits Distillation</h3><h4 id="1-1-1-Background"><a href="#1-1-1-Background" class="headerlink" title="1.1.1. Background"></a>1.1.1. Background</h4><p>假设给定一张图片，对图片中的物体/事物做识别，通常情况下只是使用简单的0/1标签来进行标注，即图中最显著的物体/事物在所属标签下标注为1,其余参数设置为0，以下图为例。在识别过程中，下图的标签为猫。然而，这样的标注忽略了：球，靠垫，猫下方的牛仔裤。</p>
<p><img src="https://i.loli.net/2021/05/23/vXFEitOnq48TlYg.png" style="width:200px;"></p>
<p>上述的标注方法就造成了标签不完整问题，但是，如果将图片中的某一块单独分割出来做识别，以球所在块为例，那就会造成识别结果与原来标注的情况不一致的问题。</p>
<p><img src="https://i.loli.net/2021/05/23/FGIC4XzPNubBgc3.png" style="width:400px;"></p>
<p>解决办法: <strong>Label Refinery</strong></p>
<p>即: 用自身Distill自身，过程如下:</p>
<p><img src="https://i.loli.net/2021/05/23/8ah5wqeA4XGWnvT.png" style="width:500px;"></p>
<h4 id="1-1-2-Deep-Mutual-Learning"><a href="#1-1-2-Deep-Mutual-Learning" class="headerlink" title="1.1.2. Deep Mutual Learning"></a>1.1.2. Deep Mutual Learning</h4><p>同时训练两个网络，互相学习对方的logits.</p>
<p><img src="https://i.loli.net/2021/05/23/SqbklAcxIRh67d4.png" style="width:500px;"></p>
<p>Step 1: 让<code>$y_{2,t}$</code>对<code>$y_{1,t}$</code>做diverse，然后与CE的值进行相加得到用于更新Network 1参数的新的<code>$loss$</code>。</p>
<p><img src="https://i.loli.net/2021/05/23/mUXodnkbNyKaHLr.png" style="width:400px;"></p>
<p>Step 2: 使用和Step 1中一模一样的方法来更新Network 2。</p>
<p><img src="https://i.loli.net/2021/05/23/Gt3ideg5OZUFkER.png" style="width:400px;"></p>
<p>使用这种方法来训练的 Network 1  和Network 2，效果都要比原先单独训练的 Network 1 和 Network 2要好。</p>
<h4 id="1-1-3-Born-Again-Neural-Networks"><a href="#1-1-3-Born-Again-Neural-Networks" class="headerlink" title="1.1.3. Born Again Neural Networks"></a>1.1.3. Born Again Neural Networks</h4><p>这种方法与Label Refinery很相似，如下图所示：</p>
<p><img src="https://i.loli.net/2021/05/23/TNA1wcrM9S4XPO2.png" style="width:400px;"></p>
<p>该方法与Label Refinery的差别在于：</p>
<ul>
<li>初始的Model是通过KD得到的.</li>
<li>迭代使用CE.</li>
<li>需要Ensemble所有的Student Model.</li>
</ul>
<h4 id="1-1-4-Pure-Logits-KD中的问题"><a href="#1-1-4-Pure-Logits-KD中的问题" class="headerlink" title="1.1.4. Pure Logits KD中的问题"></a>1.1.4. Pure Logits KD中的问题</h4><p><strong>小模型无法学习大模型的knowledge。</strong></p>
<p>解决办法：使用一个参数量介于Teach&amp;Student的TA做中间人来帮助Student学习，以此避免model差距过大学不好。</p>
<p><img src="https://i.loli.net/2021/05/23/5EGOWPacpUksY2j.png" style="width:400px;"></p>
<h3 id="1-2-Feature-Distillation"><a href="#1-2-Feature-Distillation" class="headerlink" title="1.2. Feature Distillation"></a>1.2. Feature Distillation</h3><h4 id="1-2-1-Background"><a href="#1-2-1-Background" class="headerlink" title="1.2.1 Background"></a>1.2.1 Background</h4><p>Feature的核心思路即让Student Net去“看到”Teach Net所捕获的特征，以此来学习生成Teacher Net中的Feature进而得到正确的结果，这个过程的示意图如下。</p>
<p><img src="https://i.loli.net/2021/05/23/sYwSP2qvriKVf1l.png" style="width:400px;"></p>
<h4 id="1-2-2-FitNet"><a href="#1-2-2-FitNet" class="headerlink" title="1.2.2 FitNet"></a>1.2.2 FitNet</h4><p>具体流程：先让Student Net学习如何产生Teacher Net的中间Feature,之后再使用Baseline KD.</p>
<p><img src="https://i.loli.net/2021/05/23/i8yJxVszpYjcG9g.png" style="width:400px;"></p>
<p>

</p><p><strong>Step 1: Fit feature</strong> 如下图所示，Teacher Net包含7层，Student Net包含4层，当前的目标是要让Student Net的前3层网络生成与Teacher Net中前4层网络相同的特征矩阵。因此，对Student Net第三层网络的输出加上一个权重矩阵<code>$W_r$</code>使其能够生成和Teacher Net第4层输出特征达到一定程度的相似。</p>
<p><img src="https://i.loli.net/2021/05/23/IMczHJLQhd5P1Dr.png" style="width:400px;"></p>
<p><strong>Step 2: Fit logits</strong> 直接让Student Net的输出<code>$y_s$</code>去学习Teacher Net的输出<code>$y_t$</code>，如下图所示。</p>
<blockquote>
<p>注: 模型架构越相近，效果越好。</p>
</blockquote>
<p><img src="https://i.loli.net/2021/05/23/jglL4I85nqp3YVS.png" style="width:400px;"></p>
<h4 id="1-2-3-FitNet中存在的问题-amp-解决"><a href="#1-2-3-FitNet中存在的问题-amp-解决" class="headerlink" title="1.2.3. FitNet中存在的问题&amp;解决"></a>1.2.3. FitNet中存在的问题&amp;解决</h4><p><strong>Student Net不能很好的理解Teacher Net学到的Features.</strong></p>
<p><img src="https://i.loli.net/2021/05/23/K4nUwYSkWhZoceR.png" style="width:400px;"></p>
<p>造成问题的原因：</p>
<ul>
<li>模型容量上的区别.</li>
<li>Teacher Net上有大量冗余.</li>
</ul>
<p>解决思路：通过Knowledge Compression减少模型冗余. 如下图所示.</p>
<p><img src="https://i.loli.net/2021/05/23/DuqPHK7gf6r1BOG.png" style="width:400px;"></p>
<p><strong>解决方法：Attention.</strong></p>
<p>Teacher Net每经过3层网络（1个group）就生成一个Attention，Student Net分3次去学习Teacher Net生成的Attention. 如下图所示.</p>
<p><img src="https://i.loli.net/2021/05/23/DFKlTIoxfn5CquS.png" style="width:500px;"></p>
<p>

</p><p><strong>Attention Map的生成过程：</strong> 将<code>$(W,H,C)$</code>的<code>$weight$</code>各自平方后加成<code>$(W,H)$</code>的矩阵<code>$T$</code>，<code>$Attention Map=T/norm(M)$</code>.</p>
<p><img src="https://i.loli.net/2021/05/23/ya2HRIsqZV3idmQ.png" style="width:400px;"></p>
<h3 id="1-3-Relation-Distillation"><a href="#1-3-Relation-Distillation" class="headerlink" title="1.3. Relation Distillation"></a>1.3. Relation Distillation</h3><h4 id="1-3-1-Background"><a href="#1-3-1-Background" class="headerlink" title="1.3.1. Background"></a>1.3.1. Background</h4><p><img src="https://i.loli.net/2021/05/23/3HkEhNT8R7m4KPb.png" style="width:300px;"></p>
<p>Individual KD: 以每个sample为单位做知识蒸馏.<br><br>Relational KD: 以每个sample之间的关系做知识蒸馏.</p>
<p>Relational KD表示Student Net学习Teacher Net之间的关系模式.</p>
<p><img src="https://i.loli.net/2021/05/23/52UlhIKdLcYnvqA.png" style="width:500px;"></p>
<p>

</p><p>结构性质的Relational KD示意图如下所示:</p>
<p><img src="https://i.loli.net/2021/05/23/L1zb7FY46flaWsC.png" style="width:500px;"></p>
<p>在Angle-wise KD上，只要在cosine上相似即可。</p>
<h3 id="1-3-2-Distill-Relational-Information-between-Feature"><a href="#1-3-2-Distill-Relational-Information-between-Feature" class="headerlink" title="1.3.2. Distill Relational Information between Feature"></a>1.3.2. Distill Relational Information between Feature</h3><p>以下图为例，需要识别的图片包括：0,9,1.三张图片经过一个小型网络之后生成了两类特征：circle和vertical line.0,9,1三个数字在这两个特征的维度分别是：[1,0],[1,1],[0,1]，通过cosine相似度可以计算出这三个特征之间的关系。cosine相似度计算公式如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cos=\frac&#123;\sum(x_i \times y_i)&#125;&#123;\sqrt(\sum(x_i^2)) \times \sqrt(\sum(y_i^2))&#125;</span><br></pre></td></tr></table></figure></p>
<p>基于上述计算公式，可以得到三张图片之间的cosine相似度。</p>
<p><img src="https://i.loli.net/2021/05/23/eCabWYMsZV1JD68.png" style="width:400px;"></p>
<p>基于这种方法Student Net就需要去学习Teacher Net生成features的方法，即，去学习cosine之间的similarity.</p>
<p><img src="https://i.loli.net/2021/05/23/7zs2OSXUGuoTjQg.png" style="width:400px;"></p>
<p>完整流程图如下图所示，从Teacher Net和Student Net中分别抽取出一个以batch为单位的Activations（压缩为1d数据），乘以权重除以Norm得到Pairwise Similarities，再让两个Pairwise Similarities产生loss，Student Net就可以学到Teacher Net的knowledge.</p>
<p><img src="https://i.loli.net/2021/05/23/SXakzRMHjIsyofx.png" style="width:400px;"></p>
<h2 id="2-Network-Pruning"><a href="#2-Network-Pruning" class="headerlink" title="2. Network Pruning"></a>2. Network Pruning</h2><h3 id="2-1-Background"><a href="#2-1-Background" class="headerlink" title="2.1. Background"></a>2.1. Background</h3><p>Neuron Pruning 流程图如下所示，假如要去掉中间某个层的神经元，只需要改变计算矩阵的维度即可。</p>
<p><img src="https://i.loli.net/2021/05/23/edxjPAawOFLKTvu.png" style="width:400px;"></p>
<p>Neuron Pruning在CNN上的应用与上图同理，区别在于CNN的应用上多了一个维度：channels。示意图如下。</p>
<p><img src="https://i.loli.net/2021/05/23/THC75AEKtvIjZMS.png" style="width:400px;"></p>
<h3 id="2-2-Evaluate-by-Weight"><a href="#2-2-Evaluate-by-Weight" class="headerlink" title="2.2. Evaluate by Weight"></a>2.2. Evaluate by Weight</h3><h4 id="2-2-1-sum-of-L1-norm"><a href="#2-2-1-sum-of-L1-norm" class="headerlink" title="2.2.1. sum of L1-norm"></a>2.2.1. sum of L1-norm</h4><p>使用L1-norm的方式计算权重来决定Prune哪些权重，流程图如下.同理，也可以使用L2-norm的方式来进行计算。</p>
<p><img src="https://i.loli.net/2021/05/23/2c9JCWVKitg6LeQ.png" style="width:400px;"></p>
<p><br><br><br></p>
<p>L_n-norm的计算公式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">||x||_p=(\sum_&#123;i=1&#125;^n(|x_i|^p))^&#123;\frac&#123;1&#125;&#123;p&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="2-2-2-Filter-Pruning-via-Geometric-Media-FPGA"><a href="#2-2-2-Filter-Pruning-via-Geometric-Media-FPGA" class="headerlink" title="2.2.2. Filter Pruning via Geometric Media - FPGA"></a>2.2.2. Filter Pruning via Geometric Media - FPGA</h4><p>不同的norm会有不同的filters情况，在norm参数上设定一个阈值T，Prune掉小于阈值T上的全部norm所对应的neuron即可。</p>
<p><img src="https://i.loli.net/2021/05/23/tdchXWmkCr9xsH5.png" style="width:350px;"></p>
<p><strong>存在的问题：</strong></p>
<ul>
<li>Value of norm的值域范围小，难以找到合适的阈值；</li>
<li>Vlaue of norm不接近0，所有的feature map都对结果有贡献，也就不能进行Prune.</li>
</ul>
<p><img src="https://i.loli.net/2021/05/23/AIfVgxznNWDTdvX.png" style="width:400px;"></p>
<p><br><br><br></p>
<p><strong>解决办法：</strong> 使用几何中心的方法进行Prune. 思路：其中多个filters或许具有相同的作用，因此，即使Prune掉其中的一个或几个filters并不会对最终的结果产生影响。</p>
<p><img src="https://i.loli.net/2021/05/23/d2OV5Sga6TNeiqz.png" style="width:400px;"></p>
<p><br><br><br></p>
<p>找到几何中心，保留，删除其余的neuron.</p>
<p><img src="https://i.loli.net/2021/05/23/r1YvwBEpyzdcq3Z.png" style="width:400px;"></p>
<h4 id="2-2-3-Network-Slimming"><a href="#2-2-3-Network-Slimming" class="headerlink" title="2.2.3. Network Slimming"></a>2.2.3. Network Slimming</h4><p>使用对BN进行调整的<code>$\gamma$</code>参数来评估参数重要性.</p>
<p><img src="https://i.loli.net/2021/05/23/T8pnQHjaCuWE4IG.png" style="width:400px;"></p>
<p><br><br><br></p>
<p>然而，如果对<code>$\gamma$</code>不加以限制，<code>$\gamma$</code>的分布会导致网络难以Prune，因为很多<code>$\gamma$</code>是non-trivial(对网络均有贡献). 在对<code>$\gamma$</code>做限制之后，其值大量集中在0附近，有利于做Prune，直接Prune掉某个比0大一点的阈值即可.</p>
<p><img src="https://i.loli.net/2021/05/23/nqktzL6Wur5BxpI.png" style="width:400px;"></p>
<h4 id="2-2-4-Average-Percentage-of-Zeros"><a href="#2-2-4-Average-Percentage-of-Zeros" class="headerlink" title="2.2.4. Average Percentage of Zeros"></a>2.2.4. Average Percentage of Zeros</h4><p>在网络的前向传播过程中，经过Conv和ReLU计算之后，会有一些Feature变成0，将所有的非0值加起来做Average.</p>
<p><img src="https://i.loli.net/2021/05/23/FodLXxGHzf7MQiS.png" style="width:400px;"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><p>Knowledge Distillation</p>
<ul>
<li>Distilling the Knowledge in a Neural Network (NIPS 2014)</li>
<li>Deep Mutual Learning (CVPR 2018)</li>
<li>Born Again Neural Networks (ICML 2018)</li>
<li>Label Refinery: Improving ImageNet Classification through Label Progression</li>
<li>Improved Knowledge Distillation via Teacher Assistant (AAAI 2020)</li>
<li>FitNets : Hints for Thin Deep Nets (ICLR2015)</li>
<li>Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer (ICLR 2017)</li>
<li>Relational Knowledge Distillation (CVPR 2019)</li>
<li>Similarity-Preserving Knowledge Distillation (ICCV 2019)</li>
</ul>
</li>
<li><p>Network Pruning</p>
<ul>
<li>Pruning Filters for Efficient ConvNets (ICLR 2017)</li>
<li>Learning Efficient Convolutional Networks Through Network Slimming (ICCV 2017)</li>
<li>Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (CVPR 2019)</li>
<li>Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures</li>
<li>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (ICLR 2019)</li>
<li>Rethinking the value of network pruning (ICLR 2019)</li>
<li>Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask (ICML 2019)</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/05/23/Model-Compression-基本概念/" data-id="cktfedoxi001hf4uk2ao0ypf0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Model-Compression/">Model Compression</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/06/02/Why-Deep-Structure/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Why Deep Structure
        
      </div>
    </a>
  
  
    <a href="/2020/05/21/CSS在开发中的基本操作小结/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">CSS在开发中的基本操作小结</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
    <h3 class="widget-title">联系方式</h3>
    <div class="widget">
	  <li><a>klausvon@163.com</a></li>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">目录</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CTF/">CTF</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/HTML/">HTML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP-知识图谱/">NLP,知识图谱</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/VQA/">VQA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/XSS漏洞学习/">XSS漏洞学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/代码审计/">代码审计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/开发/">开发</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学学习/">数学学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/漏洞学习/">漏洞学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/环境配置/">环境配置</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/">神经网络</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CTF-Web/">CTF_Web</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HTML/">HTML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JavaScript/">JavaScript</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Model-Compression/">Model Compression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PHP/">PHP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL-injection/">SQL_injection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VQA/">VQA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XXE/">XXE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/凸包算法/">凸包算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/开发/">开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数学学习/">数学学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据挖掘/">数据挖掘</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/环境配置/">环境配置</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/知识图谱/">知识图谱</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 14.29px;">Algorithm</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CTF-Web/" style="font-size: 18.57px;">CTF_Web</a> <a href="/tags/HTML/" style="font-size: 10px;">HTML</a> <a href="/tags/JavaScript/" style="font-size: 10px;">JavaScript</a> <a href="/tags/ML/" style="font-size: 11.43px;">ML</a> <a href="/tags/Model-Compression/" style="font-size: 10px;">Model Compression</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/PHP/" style="font-size: 10px;">PHP</a> <a href="/tags/Python/" style="font-size: 11.43px;">Python</a> <a href="/tags/SQL-injection/" style="font-size: 12.86px;">SQL_injection</a> <a href="/tags/VQA/" style="font-size: 10px;">VQA</a> <a href="/tags/XXE/" style="font-size: 10px;">XXE</a> <a href="/tags/凸包算法/" style="font-size: 10px;">凸包算法</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a> <a href="/tags/开发/" style="font-size: 18.57px;">开发</a> <a href="/tags/数学学习/" style="font-size: 11.43px;">数学学习</a> <a href="/tags/数据挖掘/" style="font-size: 11.43px;">数据挖掘</a> <a href="/tags/环境配置/" style="font-size: 10px;">环境配置</a> <a href="/tags/知识图谱/" style="font-size: 17.14px;">知识图谱</a> <a href="/tags/神经网络/" style="font-size: 15.71px;">神经网络</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/07/31/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2021/06/03/Basic-Understanding-of-Optimization/">Basic Understanding of Optimization</a>
          </li>
        
          <li>
            <a href="/2021/06/02/Why-Deep-Structure/">Why Deep Structure</a>
          </li>
        
          <li>
            <a href="/2021/05/23/Model-Compression-基本概念/">What is Model Compression</a>
          </li>
        
          <li>
            <a href="/2020/05/21/CSS在开发中的基本操作小结/">CSS在开发中的基本操作小结</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
	 <a class="theme-link"  href="https://mteacher.top/"> martini's blog </a><span>&nbsp;&nbsp;</span>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Klaus<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>